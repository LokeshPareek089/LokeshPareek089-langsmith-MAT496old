{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized âœ…\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads from local .env by default\n",
    "\n",
    "import os\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from groq import Groq\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "\n",
    "print(\"Groq client initialized âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-44929bca-c5bd-4a25-ab0c-71f12e0be787', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\" It's a lovely spot with a cozy atmosphere and delicious food. Would you like to book a table for two there?\", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1759248105, model='llama-3.1-8b-instant', object='chat.completion', system_fingerprint='fp_90c2e79dab', usage=CompletionUsage(completion_tokens=25, prompt_tokens=69, total_tokens=94, completion_time=0.039076776, prompt_time=0.004290237, queue_time=0.044696463, total_time=0.043367013), usage_breakdown=None, x_groq={'id': 'req_01k6dnqfjjeavswcwgcm6zyrrp'}, service_tier='on_demand')\n"
     ]
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant. ðŸ¦œ\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n",
    "]\n",
    "\n",
    "@traceable(\n",
    "  run_type=\"llm\",\n",
    "  metadata={\"ls_provider\": \"groq\", \"ls_model_name\": \"llama-3.1-8b-instant\"}\n",
    ")\n",
    "def chat_model(messages: list):\n",
    "  # Defensive: ensure messages is a list of dicts with 'role' and 'content'\n",
    "  if not isinstance(messages, list) or not all(isinstance(m, dict) and 'role' in m and 'content' in m for m in messages):\n",
    "      raise ValueError(\"messages must be a list of dicts with 'role' and 'content' keys\")\n",
    "  user_message = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), \"\")\n",
    "  if \"table\" in user_message.lower():\n",
    "      suggestion = \"How about The Parrot Bistro? ðŸ¦œðŸ½ï¸\"\n",
    "      # Defensive: avoid mutating input list\n",
    "      new_messages = messages + [{\"role\": \"assistant\", \"content\": suggestion}]\n",
    "      return groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=new_messages,\n",
    "        temperature=0.3,\n",
    "      )\n",
    "  return groq_client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0.3,\n",
    "  )\n",
    "\n",
    "try:\n",
    "    response = chat_model(inputs)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'choices': [{'message': {'content': 'Why did the parrot go to the restaurant? ',\n",
       "     'role': 'assistant'}}]},\n",
       " {'choices': [{'message': {'content': 'Because it wanted a cracker with its dinner! ðŸ¦œðŸ¥¨',\n",
       "     'role': 'assistant'}}]}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _reduce_chunks(chunks: list):\n",
    "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
    "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"groq\", \"ls_model_name\": \"llama-3.1-8b-instant\"},\n",
    "    # TODO: Add a reduce_fn\n",
    ")\n",
    "def my_streaming_chat_model(messages: list):\n",
    "    # Stream a multi-part parrot joke\n",
    "    joke_parts = [\n",
    "        \"Why did the parrot go to the restaurant? \",\n",
    "        \"Because it wanted a cracker with its dinner! ðŸ¦œðŸ¥¨\"\n",
    "    ]\n",
    "    for chunk in joke_parts:\n",
    "        yield {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"content\": chunk,\n",
    "                        \"role\": \"assistant\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "list(\n",
    "    my_streaming_chat_model(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\n",
    "            {\"role\": \"user\", \"content\": \"polly the parrot\"},\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'Document contents 1',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar', 'fun_fact': 'Parrots can mimic human speech! ðŸ¦œ'}},\n",
       " {'page_content': 'Document contents 2',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar', 'fun_fact': 'Some parrots live over 60 years.'}},\n",
       " {'page_content': 'Document contents 3',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar',\n",
       "   'fun_fact': 'Parrots are highly intelligent birds.'}}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "def _convert_docs(results):\n",
    "  # Add a fun parrot fact to each document's metadata\n",
    "  fun_facts = [\n",
    "      \"Parrots can mimic human speech! ðŸ¦œ\",\n",
    "      \"Some parrots live over 60 years.\",\n",
    "      \"Parrots are highly intelligent birds.\"\n",
    "  ]\n",
    "  return [\n",
    "      {\n",
    "          \"page_content\": r,\n",
    "          \"type\": \"Document\",\n",
    "          \"metadata\": {\"foo\": \"bar\", \"fun_fact\": fun_facts[i % len(fun_facts)]}\n",
    "      }\n",
    "      for i, r in enumerate(results)\n",
    "  ]\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_docs(query):\n",
    "  # Retriever returning hardcoded dummy documents.\n",
    "  # In production, this could be a real vector datatabase or other document index.\n",
    "  contents = [\"Document contents 1\", \"Document contents 2\", \"Document contents 3\"]\n",
    "  return _convert_docs(contents)\n",
    "\n",
    "retrieve_docs(\"User query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-de628041-954a-46be-a5df-c5ce263a303b', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Unfortunately, I'm a large language model, I don't have real-time access to current weather conditions. The information I provided was just a mock output.\\n\\nTo get the current weather in New York City, I suggest checking a reliable weather website or app, such as AccuWeather, Weather.com, or the National Weather Service (NWS).\\n\\nHowever, if you want to know the typical weather conditions for a specific time of year in New York City, I'd be happy to help with that.\", role='assistant', executed_tools=None, function_call=None, reasoning=None, tool_calls=None))], created=1759248106, model='llama-3.1-8b-instant', object='chat.completion', system_fingerprint='fp_50a6be1b6f', usage=CompletionUsage(completion_tokens=101, prompt_tokens=107, total_tokens=208, completion_time=0.160485031, prompt_time=0.005847562, queue_time=0.044949298, total_time=0.166332593), usage_breakdown=None, x_groq={'id': 'req_01k6dnqgmkeavttajt6qxnac80'}, service_tier='on_demand')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from groq import Groq\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "\n",
    "@traceable(\n",
    "  # TODO: Add an run_type=\"tool\"\n",
    ")\n",
    "def get_current_temperature(location: str, unit: str):\n",
    "    return 65 if unit == \"Fahrenheit\" else 17\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_groq(\n",
    "    messages: List[dict], tools: Optional[List[dict]]\n",
    ") -> str:\n",
    "  return groq_client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    tools=tools\n",
    "  )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def ask_about_the_weather(inputs, tools):\n",
    "  response = call_groq(inputs, tools)\n",
    "  tool_call_args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "  location = tool_call_args[\"location\"]\n",
    "  unit = tool_call_args[\"unit\"]\n",
    "  tool_response_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "        \"temperature\": get_current_temperature(location, unit),\n",
    "    }),\n",
    "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
    "  }\n",
    "  inputs.append(response.choices[0].message)\n",
    "  inputs.append(tool_response_message)\n",
    "  output = call_groq(inputs, None)\n",
    "  return output\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_temperature\",\n",
    "        \"description\": \"Get the current temperature for a specific location\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The city and state, e.g., San Francisco, CA\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "              \"type\": \"string\",\n",
    "              \"enum\": [\"Celsius\", \"Fahrenheit\"],\n",
    "              \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"location\", \"unit\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "]\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is the weather today in New York City?\"},\n",
    "]\n",
    "\n",
    "ask_about_the_weather(inputs, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def ask_about_the_weather(inputs, tools):\n",
    "    response = call_groq(inputs, tools)\n",
    "\n",
    "    # Check if tool_calls exist in the response\n",
    "    tool_calls = getattr(response.choices[0].message, \"tool_calls\", None)\n",
    "    if not tool_calls:\n",
    "        print(\"[DEBUG] No tool calls returned by model. Full response:\")\n",
    "        print(response)\n",
    "        return {\"error\": \"No tool calls returned by model\", \"raw_response\": response}\n",
    "\n",
    "    # Extract arguments safely\n",
    "    tool_call_args = json.loads(tool_calls[0].function.arguments)\n",
    "    location = tool_call_args.get(\"location\", \"Unknown\")\n",
    "    unit = tool_call_args.get(\"unit\", \"Unknown\")\n",
    "\n",
    "    return {\"location\": location, \"unit\": unit, \"raw_response\": response}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
