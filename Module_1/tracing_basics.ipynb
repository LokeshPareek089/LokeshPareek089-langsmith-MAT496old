{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized ✅\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads from local .env by default\n",
    "\n",
    "import os\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from groq import Groq\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "\n",
    "print(\"Groq client initialized ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import traceable\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\" \n",
    "APP_VERSION = 1.1  # bumped version\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a teaching-focused assistant.\n",
    "Always explain clearly with short examples when possible.\n",
    "If unsure, admit it politely and suggest where to learn more.\n",
    "Use max three sentences.\n",
    "\"\"\"\n",
    "\n",
    "groq_client = Groq()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# TODO: Set up tracing for each function\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)   # NOTE: This is a LangChain vector db retriever, so this .invoke() call will be traced automatically\n",
    "\n",
    "\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with the `@traceable` decorator using LangSmith, you need to decorate a function with `@traceable`. \n",
      "\n",
      "For example:\n",
      "\n",
      "```python\n",
      "from langsmith import traceable\n",
      "\n",
      "@traceable\n",
      "async def my_function():\n",
      "    # your code here\n",
      "    pass\n",
      "```\n",
      "\n",
      "Additionally, make sure to set the `LANGSMITH_TRACING` environment variable to `'true'` to enable tracing.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with the @traceable decorator?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"vectordb\": \"faiss\", \"environment\": \"dev\", \"app_version\": APP_VERSION},\n",
    "    tags=[\"rag\", \"retrieval\"]\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    # TODO: replace this with your retriever logic\n",
    "    return retriever.get_relevant_documents(question)\n",
    "\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER, \"temperature\": 0.0},\n",
    "    tags=[\"llm\", \"completion\"]\n",
    ")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "\n",
    "@traceable(tags=[\"rag\", \"pipeline\"])\n",
    "def langsmith_rag(question: str, **kwargs):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    answer = response.choices[0].message.content\n",
    "    print(f\"[DEBUG] Question: {question}\\n[DEBUG] Answer: {answer}\")  # Debug logging\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_21820\\863029767.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Question: How do I add Metadata to a Run with @traceable?\n",
      "[DEBUG] Answer: To add metadata to a Run with `@traceable()`, you can pass a `metadata` dictionary as an argument to the decorator or the `hello_llm` function. Here's an example:\n",
      "\n",
      "```python\n",
      "@traceable(\n",
      "    run_type=\"llm\",\n",
      "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\", \"other_key\": \"other_value\"}\n",
      ")\n",
      "def hello_llm(prompt: str):\n",
      "    return {\n",
      "        \"choices\": [\n",
      "            {\"text\": \"Hello, \" + prompt}\n",
      "        ],\n",
      "        \"usage_metadata\": {\n",
      "            \"input_tokens\": 4,\n",
      "            \"output_tokens\": 5,\n",
      "            \"total_tokens\": 9,\n",
      "        },\n",
      "    }\n",
      "\n",
      "hello_llm(\"polly the parrot\\n\")\n",
      "```\n",
      "\n",
      "In this example, `metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\", \"other_key\": \"other_value\"}` is passed to the `@traceable()` decorator, which will associate the metadata with the Run.\n",
      "To add metadata to a Run with `@traceable()`, you can pass a `metadata` dictionary as an argument to the decorator or the `hello_llm` function. Here's an example:\n",
      "\n",
      "```python\n",
      "@traceable(\n",
      "    run_type=\"llm\",\n",
      "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\", \"other_key\": \"other_value\"}\n",
      ")\n",
      "def hello_llm(prompt: str):\n",
      "    return {\n",
      "        \"choices\": [\n",
      "            {\"text\": \"Hello, \" + prompt}\n",
      "        ],\n",
      "        \"usage_metadata\": {\n",
      "            \"input_tokens\": 4,\n",
      "            \"output_tokens\": 5,\n",
      "            \"total_tokens\": 9,\n",
      "        },\n",
      "    }\n",
      "\n",
      "hello_llm(\"polly the parrot\\n\")\n",
      "```\n",
      "\n",
      "In this example, `metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\", \"other_key\": \"other_value\"}` is passed to the `@traceable()` decorator, which will associate the metadata with the Run.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add Metadata to a Run with @traceable?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Question: How do I add metadata at runtime?\n",
      "[DEBUG] Answer: To add metadata at runtime, you can pass it as a parameter to the `invoke` method. For example:\n",
      "\n",
      "```python\n",
      "chain.invoke({\"input\": \"What is the meaning of life?\"}, {\"metadata\": {\"invoke-key\": \"invoke-value\"}})\n",
      "```\n",
      "\n",
      "This way, you can dynamically set the metadata for the invocation. Alternatively, you can update the metadata on the parent run tree as shown in the code snippet:\n",
      "\n",
      "```python\n",
      "rt = ls.get_current_run_tree()\n",
      "rt.metadata[\"some-conditional-key\"] = \"some-val\"\n",
      "```\n",
      "\n",
      "Note that the `rt` variable is obtained from the `ls.get_current_run_tree()` method, which gives you access to the current run tree. You can then update the metadata dictionary associated with it.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I add metadata at runtime?\"\n",
    "ai_answer = langsmith_rag(\n",
    "    question,\n",
    "    langsmith_extra={\"metadata\": {\"runtime_metadata\": f\"run_v{APP_VERSION}\"}}\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
