{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)  # .env is in the current Module_1 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "thread_id = uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "import os\n",
    "import random\n",
    "\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# Fun: Add a random emoji to every system prompt for a more engaging assistant!\n",
    "def get_random_emoji():\n",
    "    emojis = [\"ü§ñ\", \"üí°\", \"üìö\", \"‚ú®\", \"üß†\", \"üîç\", \"üìù\", \"üöÄ\"]\n",
    "    return random.choice(emojis)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    docs = retriever.invoke(question)\n",
    "    print(f\"[Retriever] Retrieved {len(docs)} documents for: '{question}'\")\n",
    "    return docs\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = f\"\"\"{get_random_emoji()} You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the latest question in the conversation.\\nIf you don't know the answer, just say that you don't know.\\nUse three sentences maximum and keep the answer concise.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    print(f\"[System Prompt] {rag_system_prompt}\")\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = \"moonshotai/kimi-k2-instruct-0905\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    print(f\"[LLM] Calling model: {model} with {len(messages)} messages.\")\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    print(f\"[RAG] Processing question: {question}\")\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    print(f\"[RAG] Response received.\")\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Processing question: What are three creative ways to visualize data in Python?\n",
      "[Retriever] Retrieved 4 documents for: 'What are three creative ways to visualize data in Python?'\n",
      "[System Prompt] üöÄ You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
      "If you don't know the answer, just say that you don't know.\n",
      "Use three sentences maximum and keep the answer concise.\n",
      "[LLM] Calling model: moonshotai/kimi-k2-instruct-0905 with 2 messages.\n",
      "[RAG] Response received.\n",
      "üìà Creative Answer: 1. **Plot as Interactive Network**: Use `networkx` plus PyVis or Plotly to turn your refund graph into a responsive, drag-n-drop graph‚Äîhovering on nodes or edges in a Jupyter/IPython environment instantly reveals metadata without extra code.  \n",
      "2. **Sankey + State Steps**: Transform every step (`gather_info ‚Üí lookup ‚Üí END`, refund‚ÜíEND) into a Sankey diagram (`plotly Sankey`) whose colored ribbons show where users flowed (and where they aborted).  \n",
      "3. **Step-wise Bar Races**: Convert per-step timing or accuracy metrics into an animated horizontal bar chart (`matplotlib` FuncAnimation / gif-export) that plays like a ‚Äúbar race‚Äù so you can visually spot performance regressions at each graph phase.\n",
      "[RAG] Response received.\n",
      "üìà Creative Answer: 1. **Plot as Interactive Network**: Use `networkx` plus PyVis or Plotly to turn your refund graph into a responsive, drag-n-drop graph‚Äîhovering on nodes or edges in a Jupyter/IPython environment instantly reveals metadata without extra code.  \n",
      "2. **Sankey + State Steps**: Transform every step (`gather_info ‚Üí lookup ‚Üí END`, refund‚ÜíEND) into a Sankey diagram (`plotly Sankey`) whose colored ribbons show where users flowed (and where they aborted).  \n",
      "3. **Step-wise Bar Races**: Convert per-step timing or accuracy metrics into an animated horizontal bar chart (`matplotlib` FuncAnimation / gif-export) that plays like a ‚Äúbar race‚Äù so you can visually spot performance regressions at each graph phase.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are three creative ways to visualize data in Python?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"\\U0001F4C8 Creative Answer: {ai_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Processing question: Suggest a fun project that combines AI and art.\n",
      "[Retriever] Retrieved 4 documents for: 'Suggest a fun project that combines AI and art.'\n",
      "[System Prompt] ü§ñ You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
      "If you don't know the answer, just say that you don't know.\n",
      "Use three sentences maximum and keep the answer concise.\n",
      "[LLM] Calling model: moonshotai/kimi-k2-instruct-0905 with 2 messages.\n",
      "[RAG] Response received.\n",
      "üé® AI+Art Idea: Build a \"museum tour\" generative chatbot that asks visitors for three words and instantly produces an imaginary artwork‚Äôs image (DALL¬∑E or SD) plus a back-story, then grades itself by simulating how long the visitor keeps talking about the piece via multi-turn conversation simulation with `langsmith + openevals`.\n",
      "[RAG] Response received.\n",
      "üé® AI+Art Idea: Build a \"museum tour\" generative chatbot that asks visitors for three words and instantly produces an imaginary artwork‚Äôs image (DALL¬∑E or SD) plus a back-story, then grades itself by simulating how long the visitor keeps talking about the piece via multi-turn conversation simulation with `langsmith + openevals`.\n"
     ]
    }
   ],
   "source": [
    "question = \"Suggest a fun project that combines AI and art.\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(f\"\\U0001F3A8 AI+Art Idea: {ai_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
