{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized ✅\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Loads from local .env by default\n",
    "\n",
    "import os\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from groq import Groq\n",
    "groq_client = Groq(api_key=groq_key)\n",
    "\n",
    "print(\"Groq client initialized ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌐 Connected! Running the RAG graph...\n",
      "❌ An unexpected error occurred:\n",
      "❌ An unexpected error occurred:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 86, in handle_request\n",
      "    self._send_request_headers(**kwargs)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 144, in _send_request_headers\n",
      "    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 980, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\2183099806.py\", line 21, in <module>\n",
      "    result = simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "                 ^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py\", line 2657, in stream\n",
      "    for _ in runner.tick(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\3924603565.py\", line 37, in generate_response\n",
      "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 533, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1012, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "groq.APIConnectionError: Connection error.\n",
      "During task with name 'generate_response' and id '0ea7a9c5-7ce3-e627-252e-4a644c67ab50'\n"
     ]
    }
   ],
   "source": [
    "# Creative, robust RAG graph invocation with advanced error handling and user feedback\n",
    "import traceback\n",
    "import socket\n",
    "import httpx\n",
    "\n",
    "def is_connected(host=\"8.8.8.8\", port=53, timeout=3):\n",
    "    \"\"\"Check internet connectivity by attempting to connect to a DNS server.\"\"\"\n",
    "    try:\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "        socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "question = \"How do I set up tracing if I'm using LangChain?\"\n",
    "if not is_connected():\n",
    "    print(\"🚫 No internet connection. Please check your network and try again.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"🌐 Connected! Running the RAG graph...\")\n",
    "        result = simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})\n",
    "        print(\"✅ Success! Here is your result:\")\n",
    "        print(result)\n",
    "    except httpx.ConnectError as ce:\n",
    "        print(\"❌ Connection error: Unable to reach the Groq API. Please check your internet or proxy settings.\")\n",
    "        print(f\"Details: {ce}\")\n",
    "    except httpx.HTTPStatusError as he:\n",
    "        print(f\"❌ HTTP error: {he.response.status_code} - {he.response.text}\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ An unexpected error occurred:\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, trace\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "groq_client = Groq()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_groq` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "# TODO: Remove traceable, and use with trace()\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    # NOTE: Our documents came in as a list of objects, but we just want to log a string\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    # TODO: Use with trace()\n",
    "    # with trace(\n",
    "    #     name=\"Generate Response\",\n",
    "    #     run_type=\"chain\", \n",
    "    #     inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "    #     metadata={\"foo\": \"bar\"},\n",
    "    # ) as ls_trace:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    response = call_groq(messages)\n",
    "    # TODO: End your trace and write outputs to LangSmith\n",
    "    # ls_trace.end(outputs={\"output\": response})\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "call_groq\n",
    "- Returns the chat completion output from Groq\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_groq(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 86, in handle_request\n",
      "    self._send_request_headers(**kwargs)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 144, in _send_request_headers\n",
      "    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 980, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\2119114042.py\", line 6, in <module>\n",
      "    ai_answer = langsmith_rag(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\1632265551.py\", line 85, in langsmith_rag\n",
      "    response = generate_response(question, documents)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\1632265551.py\", line 56, in generate_response\n",
      "    response = call_groq(messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\1632265551.py\", line 69, in call_groq\n",
      "    response = groq_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1012, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "groq.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "# Run langsmith_rag and print the answer with error handling\n",
    "import traceback\n",
    "\n",
    "question = \"How do I trace with tracing context?\"\n",
    "try:\n",
    "    ai_answer = langsmith_rag(question)\n",
    "    print(ai_answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import wrap_groq (if available in your tracing library)\n",
    "# from langsmith.wrappers import wrap_groq\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Wrap the Groq Client (if needed)\n",
    "groq_client = Groq()\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # TODO: We don't need to use @traceable on a nested function call anymore,\n",
    "    # wrap_groq takes care of this for us (if available)\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable\n",
    "def call_groq(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    return groq_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_groq(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Connection error.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 86, in handle_request\n",
      "    self._send_request_headers(**kwargs)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 144, in _send_request_headers\n",
      "    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 980, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\2987531052.py\", line 6, in <module>\n",
      "    ai_answer = langsmith_rag_with_wrap_groq(question)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\35722367.py\", line 56, in langsmith_rag_with_wrap_groq\n",
      "    response = generate_response(question, documents)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\35722367.py\", line 42, in generate_response\n",
      "    return call_groq(messages)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 693, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\35722367.py\", line 48, in call_groq\n",
      "    return groq_client.chat.completions.create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1012, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "groq.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "# Run langsmith_rag_with_wrap_groq and print the answer with error handling\n",
    "import traceback\n",
    "\n",
    "question = \"How do I trace with wrap_groq?\"\n",
    "try:\n",
    "    ai_answer = langsmith_rag_with_wrap_groq(question)\n",
    "    print(ai_answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Completions.create() got an unexpected keyword argument 'langsmith_extra'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\11903616.py\", line 12, in <module>\n",
      "    response = groq_client.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Completions.create() got an unexpected keyword argument 'langsmith_extra'\n"
     ]
    }
   ],
   "source": [
    "# Minimal, robust Groq API call with error handling\n",
    "import traceback\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What color is the sky?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        langsmith_extra={\"metadata\": {\"foo\": \"bar\"}}\n",
    "    )\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to set API keys here. They are loaded from .env automatically by dotenv in the first code cell.\n",
    "# If you need to override, edit your .env file or set os.environ before importing clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled() # This should return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunTree-based RAG pipeline with error handling\n",
    "import traceback\n",
    "\n",
    "from langsmith import RunTree\n",
    "from groq import Groq\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "groq_client = Groq()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    try:\n",
    "        child_run = parent_run.create_child(\n",
    "            name=\"Retrieve Documents\",\n",
    "            run_type=\"retriever\",\n",
    "            inputs={\"question\": question},\n",
    "        )\n",
    "        documents = retriever.invoke(question)\n",
    "        child_run.end(outputs={\"documents\": documents})\n",
    "        child_run.post()\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieve_documents: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    try:\n",
    "        formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "        rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        \"\"\"\n",
    "        child_run = parent_run.create_child(\n",
    "            name=\"Generate Response\",\n",
    "            run_type=\"chain\",\n",
    "            inputs={\"question\": question, \"documents\": documents},\n",
    "        )\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": rag_system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "            }\n",
    "        ]\n",
    "        groq_response = call_groq(child_run, messages)\n",
    "        child_run.end(outputs={\"groq_response\": groq_response})\n",
    "        child_run.post()\n",
    "        return groq_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def call_groq(parent_run: RunTree, messages: List[dict], model: str = \"llama-3.1-8b-instant\", temperature: float = 0.0):\n",
    "    try:\n",
    "        child_run = parent_run.create_child(\n",
    "            name=\"Groq Call\",\n",
    "            run_type=\"llm\",\n",
    "            inputs={\"messages\": messages},\n",
    "        )\n",
    "        groq_response = groq_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        child_run.end(outputs={\"groq_response\": groq_response})\n",
    "        child_run.post()\n",
    "        return groq_response\n",
    "    except Exception as e:\n",
    "        print(f\"Error in call_groq: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    try:\n",
    "        root_run_tree = RunTree(\n",
    "            name=\"Chat Pipeline\",\n",
    "            run_type=\"chain\",\n",
    "            inputs={\"question\": question}\n",
    "        )\n",
    "        documents = retrieve_documents(root_run_tree, question)\n",
    "        response = generate_response(root_run_tree, question, documents)\n",
    "        output = response.choices[0].message.content if response else None\n",
    "        root_run_tree.end(outputs={\"generation\": output})\n",
    "        root_run_tree.post()\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error in langsmith_rag: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in call_groq: Connection error.\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 250, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 103, in handle_request\n",
      "    return self._connection.handle_request(request)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 136, in handle_request\n",
      "    raise exc\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 86, in handle_request\n",
      "    self._send_request_headers(**kwargs)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py\", line 144, in _send_request_headers\n",
      "    with map_exceptions({h11.LocalProtocolError: LocalProtocolError}):\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 980, in request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_client.py\", line 1014, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 249, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.LocalProtocolError: Illegal header value b'Bearer '\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lokesh Pareek\\AppData\\Local\\Temp\\ipykernel_17156\\1383979073.py\", line 69, in call_groq\n",
      "    groq_response = groq_client.chat.completions.create(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 456, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\MAT 496\\LangSmith\\HW\\venv\\Lib\\site-packages\\groq\\_base_client.py\", line 1012, in request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "groq.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
