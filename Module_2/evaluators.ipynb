{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ­ Meme Quality Evaluators\n",
    "### Because someone needs to judge internet culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high-level, a meme evaluator is like a Reddit moderator with actual standards. It takes a meme submission, compares it against peak internet culture, and returns a dankness score.\n",
    "\n",
    "In our system, evaluators judge whether a meme caption is worthy of front-page glory or belongs in the digital trash bin. We represent this as a function that takes in a Run (the meme attempt) and an Example (legendary reference memes), and returns Feedback (brutal honesty about your meme skills)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Evaluator](../../images/evaluator.png)\n",
    "\n",
    "*Fig 1: The sacred process of meme evaluation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Simple Dankness Checker\n",
    "\n",
    "Here's a basic evaluator that checks if a meme caption hits the mark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ¨ Dankness checker initialized!\n",
      "ğŸ¯ Ready to judge meme quality with extreme prejudice\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "def check_meme_dankness(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "    \"\"\"Checks if the meme caption is objectively dank\"\"\"\n",
    "    user_caption = outputs.get(\"caption\")\n",
    "    legendary_caption = reference_outputs.get(\"caption\")\n",
    "    \n",
    "    # Simple exact match (because true art cannot be replicated)\n",
    "    is_dank = user_caption == legendary_caption\n",
    "    \n",
    "    return {\n",
    "        \"score\": int(is_dank) * 100, \n",
    "        \"key\": \"exact_dankness_match\",\n",
    "        \"comment\": \"Perfect replica! ğŸ”¥\" if is_dank else \"Nice try, but not quite legendary ğŸ˜…\"\n",
    "    }\n",
    "\n",
    "print(\"âœ¨ Dankness checker initialized!\")\n",
    "print(\"ğŸ¯ Ready to judge meme quality with extreme prejudice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤– AI-as-Meme-Judge Evaluation\n",
    "\n",
    "Using AI to judge memes is like asking a robot to understand why a frog on a unicycle is funny. But we're doing it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ API key loaded (totally real, trust me)\n",
      "ğŸ¨ Ready to harness AI for critical meme analysis\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-meme-judge-supreme-xyz789\"\n",
    "\n",
    "print(\"ğŸ”‘ API key loaded (totally real, trust me)\")\n",
    "print(\"ğŸ¨ Ready to harness AI for critical meme analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Loading environment secrets from the vault...\n",
      "âœ… Configuration loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"ğŸ“š Loading environment secrets from the vault...\")\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "print(\"âœ… Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Initializing AI Meme Judge v3.0...\n",
      "ğŸ“Š Structured output schema created\n",
      "ğŸ­ System prompt calibrated for maximum meme comprehension\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Meme_Quality_Score(BaseModel):\n",
    "    humor_score: int = Field(description=\"How funny is this meme from 1-10?\")\n",
    "    relatability_score: int = Field(description=\"How relatable from 1-10?\")\n",
    "    dankness_score: int = Field(description=\"Overall dankness from 1-10, where 10 is legendary tier\")\n",
    "    would_share: bool = Field(description=\"Would you share this with your group chat?\")\n",
    "    roast: str = Field(description=\"A brief, witty critique of the meme\")\n",
    "\n",
    "print(\"ğŸ§  Initializing AI Meme Judge v3.0...\")\n",
    "print(\"ğŸ“Š Structured output schema created\")\n",
    "print(\"ğŸ­ System prompt calibrated for maximum meme comprehension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Meme evaluator function compiled!\n",
      "âš¡ Ready to rate memes with surgical precision\n"
     ]
    }
   ],
   "source": [
    "def evaluate_meme_quality(inputs: dict, reference_outputs: dict, outputs: dict):\n",
    "    \"\"\"The ultimate meme quality evaluator using GPT-4o\"\"\"\n",
    "    meme_template = inputs[\"meme_template\"]\n",
    "    legendary_caption = reference_outputs[\"caption\"]\n",
    "    user_caption = outputs[\"caption\"]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a professional meme critic with a PhD in Internet Culture. \"\n",
    "                    \"Compare a user's meme caption against a legendary reference caption. \"\n",
    "                    \"Judge the humor, relatability, and overall dankness. \"\n",
    "                    \"Be honest but entertaining in your critique. Remember: not all memes can be legendary.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Meme Template: {meme_template}\\n\\nLegendary Caption: {legendary_caption}\\n\\nUser's Caption: {user_caption}\\n\\nRate this meme!\"\n",
    "            }\n",
    "        ],\n",
    "        response_format=Meme_Quality_Score,\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.parsed\n",
    "    overall_score = (result.humor_score + result.relatability_score + result.dankness_score) / 3\n",
    "    \n",
    "    return {\n",
    "        \"score\": round(overall_score, 1), \n",
    "        \"key\": \"meme_quality\",\n",
    "        \"metadata\": {\n",
    "            \"humor\": result.humor_score,\n",
    "            \"relatability\": result.relatability_score,\n",
    "            \"dankness\": result.dankness_score,\n",
    "            \"would_share\": result.would_share,\n",
    "            \"roast\": result.roast\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"ğŸ¯ Meme evaluator function compiled!\")\n",
    "print(\"âš¡ Ready to rate memes with surgical precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Testing Time!\n",
    "\n",
    "Let's test our evaluator with a deliberately mediocre meme caption. Prepare for disappointment! ğŸ“‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ Running evaluation...\n",
      "\n",
      "ğŸ“Š MEME EVALUATION RESULTS:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Overall Score: 4.3/10 âš ï¸\n",
      "\n",
      "Breakdown:\n",
      "  ğŸ˜‚ Humor: 3/10\n",
      "  ğŸ¤ Relatability: 5/10\n",
      "  ğŸ”¥ Dankness: 5/10\n",
      "  ğŸ“¤ Would Share: False\n",
      "\n",
      "ğŸ’¬ Critic's Roast:\n",
      "\"Your caption is giving 'I just learned about debugging yesterday' energy. The legendary version captures the existential crisis of senior devs, while yours sounds like it was written by a motivational poster. Not bad, just... basic.\"\n",
      "\n",
      "Verdict: Needs more suffering and dark humor. Try again! ğŸ’€\n"
     ]
    }
   ],
   "source": [
    "# Reference meme from the Hall of Fame\n",
    "inputs = {\n",
    "    \"meme_template\": \"Distracted Boyfriend\"\n",
    "}\n",
    "\n",
    "reference_outputs = {\n",
    "    \"caption\": \"Boyfriend: Me | Girlfriend: Working code | Other girl: Debugging for 6 hours\"\n",
    "}\n",
    "\n",
    "# User's attempt (deliberately mid)\n",
    "outputs = {\n",
    "    \"caption\": \"Boyfriend: Me | Girlfriend: Finished project | Other girl: New bug appeared\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ¬ Running evaluation...\\n\")\n",
    "evaluation_result = evaluate_meme_quality(inputs, reference_outputs, outputs)\n",
    "\n",
    "print(\"ğŸ“Š MEME EVALUATION RESULTS:\")\n",
    "print(\"â”\" * 31)\n",
    "print(f\"Overall Score: {evaluation_result['score']}/10 âš ï¸\\n\")\n",
    "print(\"Breakdown:\")\n",
    "print(f\"  ğŸ˜‚ Humor: {evaluation_result['metadata']['humor']}/10\")\n",
    "print(f\"  ğŸ¤ Relatability: {evaluation_result['metadata']['relatability']}/10\")\n",
    "print(f\"  ğŸ”¥ Dankness: {evaluation_result['metadata']['dankness']}/10\")\n",
    "print(f\"  ğŸ“¤ Would Share: {evaluation_result['metadata']['would_share']}\\n\")\n",
    "print(\"ğŸ’¬ Critic's Roast:\")\n",
    "print(f'\"{evaluation_result[\"metadata\"][\"roast\"]}\"')\n",
    "print(\"\\nVerdict: Needs more suffering and dark humor. Try again! ğŸ’€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Alternative Evaluator: Direct Run/Example Access\n",
    "\n",
    "For when you need to dig deeper into the meme metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Advanced evaluator v2 initialized\n",
      "ğŸ“¦ Now with direct access to Run and Example objects!\n"
     ]
    }
   ],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def evaluate_meme_quality_v2(root_run: Run, example: Example):\n",
    "    \"\"\"Advanced meme evaluator with full access to run metadata\"\"\"\n",
    "    meme_template = example[\"inputs\"][\"meme_template\"]\n",
    "    legendary_caption = example[\"outputs\"][\"caption\"]\n",
    "    user_caption = root_run[\"outputs\"][\"caption\"]\n",
    "    \n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {   \n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a professional meme critic with a PhD in Internet Culture. \"\n",
    "                    \"Compare a user's meme caption against a legendary reference caption. \"\n",
    "                    \"Judge the humor, relatability, and overall dankness. \"\n",
    "                    \"Be honest but entertaining in your critique.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Template: {meme_template}\\n\\nLegendary: {legendary_caption}\\n\\nUser's: {user_caption}\"\n",
    "            }\n",
    "        ],\n",
    "        response_format=Meme_Quality_Score,\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.parsed\n",
    "    overall_score = (result.humor_score + result.relatability_score + result.dankness_score) / 3\n",
    "    \n",
    "    return {\n",
    "        \"score\": round(overall_score, 1), \n",
    "        \"key\": \"meme_quality_v2\",\n",
    "        \"metadata\": {\n",
    "            \"humor\": result.humor_score,\n",
    "            \"relatability\": result.relatability_score,\n",
    "            \"dankness\": result.dankness_score,\n",
    "            \"would_share\": result.would_share,\n",
    "            \"roast\": result.roast\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"ğŸ”§ Advanced evaluator v2 initialized\")\n",
    "print(\"ğŸ“¦ Now with direct access to Run and Example objects!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸª Testing v2 evaluator with sample data...\n",
      "\n",
      "ğŸ“Š MEME EVALUATION v2 RESULTS:\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
      "Overall Score: 3.0/10 ğŸ˜¬\n",
      "\n",
      "Breakdown:\n",
      "  ğŸ˜‚ Humor: 2/10\n",
      "  ğŸ¤ Relatability: 4/10\n",
      "  ğŸ”¥ Dankness: 3/10\n",
      "  ğŸ“¤ Would Share: False\n",
      "\n",
      "ğŸ’¬ Critic's Savage Take:\n",
      "\"This is what happens when someone describes a meme instead of making one. You've captured the spirit of a Wikipedia article about memes, not an actual meme. The legendary version has layers - it's self-aware, it's meta. Yours is just... there. Like a participation trophy in meme form. 3/10, would not recommend to the group chat.\"\n",
      "\n",
      "ğŸ¯ Status: REJECTED by the Meme Council\n",
      "ğŸ’¡ Recommendation: Study the ancient texts (r/memes) and try again\n"
     ]
    }
   ],
   "source": [
    "sample_run = {\n",
    "    \"name\": \"User Meme Submission #42069\",\n",
    "    \"inputs\": {\n",
    "        \"meme_template\": \"Drake Hotline Bling\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"caption\": \"Top: Bad code | Bottom: Good code\"\n",
    "    },\n",
    "    \"is_root\": True,\n",
    "    \"status\": \"success\",\n",
    "    \"extra\": {\n",
    "        \"metadata\": {\n",
    "            \"user_confidence\": \"very high\",\n",
    "            \"expected_upvotes\": \"1000+\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sample_example = {\n",
    "    \"inputs\": {\n",
    "        \"meme_template\": \"Drake Hotline Bling\"\n",
    "    },\n",
    "    \"outputs\": {\n",
    "        \"caption\": \"Top: Fixing the bug | Bottom: Commenting out the code that causes the bug\"\n",
    "    },\n",
    "    \"metadata\": {\n",
    "        \"tier\": \"legendary\",\n",
    "        \"upvotes\": 12500,\n",
    "        \"hall_of_fame\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸª Testing v2 evaluator with sample data...\\n\")\n",
    "evaluation_v2 = evaluate_meme_quality_v2(sample_run, sample_example)\n",
    "\n",
    "print(\"ğŸ“Š MEME EVALUATION v2 RESULTS:\")\n",
    "print(\"â”\" * 31)\n",
    "print(f\"Overall Score: {evaluation_v2['score']}/10 ğŸ˜¬\\n\")\n",
    "print(\"Breakdown:\")\n",
    "print(f\"  ğŸ˜‚ Humor: {evaluation_v2['metadata']['humor']}/10\")\n",
    "print(f\"  ğŸ¤ Relatability: {evaluation_v2['metadata']['relatability']}/10\")\n",
    "print(f\"  ğŸ”¥ Dankness: {evaluation_v2['metadata']['dankness']}/10\")\n",
    "print(f\"  ğŸ“¤ Would Share: {evaluation_v2['metadata']['would_share']}\\n\")\n",
    "print(\"ğŸ’¬ Critic's Savage Take:\")\n",
    "print(f'\"{evaluation_v2[\"metadata\"][\"roast\"]}\"')\n",
    "print(\"\\nğŸ¯ Status: REJECTED by the Meme Council\")\n",
    "print(\"ğŸ’¡ Recommendation: Study the ancient texts (r/memes) and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŠ Conclusion\n",
    "\n",
    "We've successfully built a system that uses AI to judge memes. The future is now, and it's... questioning our humor? ğŸ¤”\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Meme evaluation is serious business ğŸ“ˆ\n",
    "- AI can be a harsh critic ğŸ’€\n",
    "- Most of us probably need to level up our meme game ğŸ®\n",
    "\n",
    "*Remember: With great meme power comes great responsibility. Use these evaluators wisely!* ğŸ•·ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme-judge-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
