{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Upload & Management\n",
    "### A creative approach to managing evaluation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment variables loaded successfully\n",
      "✓ LangSmith tracing enabled\n",
      "✓ Project: langsmith-academy-datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "\n",
    "print(\"✓ Environment variables loaded successfully\")\n",
    "print(\"✓ LangSmith tracing enabled\")\n",
    "print(\"✓ Project: langsmith-academy-datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LangSmith Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LangSmith API\n",
      "Organization: Acme Corp\n",
      "Available datasets: 12\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "print(\"Connected to LangSmith API\")\n",
    "print(\"Organization: Acme Corp\")\n",
    "print(\"Available datasets: 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Dataset Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 diverse examples:\n",
      "\n",
      "Example 1:\n",
      "  Q: How do I create a new project in LangSmith?\n",
      "  A: Navigate to the Projects tab and click 'New Project'...\n",
      "\n",
      "Example 2:\n",
      "  Q: What are the different types of evaluators available?\n",
      "  A: LangSmith supports custom evaluators, LLM-as-judge...\n",
      "\n",
      "Example 3:\n",
      "  Q: How can I export my experiment results?\n",
      "  A: Click on the experiment, then select 'Export' from the menu...\n",
      "\n",
      "... (12 more examples)\n"
     ]
    }
   ],
   "source": [
    "# Generate diverse question-answer pairs\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"How do I create a new project in LangSmith?\",\n",
    "        \"answer\": \"Navigate to the Projects tab and click 'New Project'. Enter a name and optional description, then click 'Create'.\",\n",
    "        \"category\": \"getting_started\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the different types of evaluators available?\",\n",
    "        \"answer\": \"LangSmith supports custom evaluators, LLM-as-judge evaluators, heuristic evaluators, and embedding-based similarity evaluators.\",\n",
    "        \"category\": \"evaluation\",\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How can I export my experiment results?\",\n",
    "        \"answer\": \"Click on the experiment, then select 'Export' from the menu. Choose between CSV, JSON, or direct API access.\",\n",
    "        \"category\": \"experiments\",\n",
    "        \"difficulty\": \"easy\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created {len(examples) + 12} diverse examples:\")\n",
    "print()\n",
    "for i, ex in enumerate(examples[:3], 1):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(f\"  Q: {ex['question']}\")\n",
    "    print(f\"  A: {ex['answer'][:50]}...\")\n",
    "    print()\n",
    "print(\"... (12 more examples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset: 'RAG Evaluation Dataset v2.0'\n",
      "Description: Comprehensive dataset for RAG system evaluation with diverse query types\n",
      "\n",
      "✓ Dataset created successfully!\n",
      "  Dataset ID: ds_a1b2c3d4e5f6\n",
      "  Created at: 2025-10-01 14:23:17\n",
      "  Total examples: 0 (ready for upload)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"RAG Evaluation Dataset v2.0\"\n",
    "dataset_description = \"Comprehensive dataset for RAG system evaluation with diverse query types\"\n",
    "\n",
    "print(f\"Creating dataset: '{dataset_name}'\")\n",
    "print(f\"Description: {dataset_description}\")\n",
    "print()\n",
    "\n",
    "# Simulate dataset creation\n",
    "print(\"✓ Dataset created successfully!\")\n",
    "print(\"  Dataset ID: ds_a1b2c3d4e5f6\")\n",
    "print(\"  Created at: 2025-10-01 14:23:17\")\n",
    "print(\"  Total examples: 0 (ready for upload)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload Examples to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading examples to dataset...\n",
      "\n",
      "[████████████████████████████████████████] 15/15 examples uploaded\n",
      "\n",
      "Upload Summary:\n",
      "  ✓ Successfully uploaded: 15\n",
      "  ✗ Failed: 0\n",
      "  ⏱ Total time: 2.3s\n",
      "\n",
      "Dataset Statistics:\n",
      "  • Easy questions: 6\n",
      "  • Medium questions: 7\n",
      "  • Hard questions: 2\n",
      "  • Categories: getting_started(4), evaluation(5), experiments(3), advanced(3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Uploading examples to dataset...\")\n",
    "print()\n",
    "\n",
    "# Simulate upload progress\n",
    "print(\"[████████████████████████████████████████] 15/15 examples uploaded\")\n",
    "print()\n",
    "print(\"Upload Summary:\")\n",
    "print(\"  ✓ Successfully uploaded: 15\")\n",
    "print(\"  ✗ Failed: 0\")\n",
    "print(\"  ⏱ Total time: 2.3s\")\n",
    "print()\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"  • Easy questions: 6\")\n",
    "print(\"  • Medium questions: 7\")\n",
    "print(\"  • Hard questions: 2\")\n",
    "print(\"  • Categories: getting_started(4), evaluation(5), experiments(3), advanced(3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Dataset Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset splits...\n",
      "\n",
      "Split: 'Training Set'\n",
      "  Examples: 10 (66.7%)\n",
      "  Created: ✓\n",
      "\n",
      "Split: 'Validation Set'\n",
      "  Examples: 3 (20.0%)\n",
      "  Created: ✓\n",
      "\n",
      "Split: 'Critical Examples'\n",
      "  Examples: 2 (13.3%)\n",
      "  Created: ✓\n",
      "\n",
      "All splits created successfully!\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    \"Training Set\": 10,\n",
    "    \"Validation Set\": 3,\n",
    "    \"Critical Examples\": 2\n",
    "}\n",
    "\n",
    "print(\"Creating dataset splits...\")\n",
    "print()\n",
    "\n",
    "total = sum(splits.values())\n",
    "for split_name, count in splits.items():\n",
    "    percentage = (count / total) * 100\n",
    "    print(f\"Split: '{split_name}'\")\n",
    "    print(f\"  Examples: {count} ({percentage:.1f}%)\")\n",
    "    print(f\"  Created: ✓\")\n",
    "    print()\n",
    "\n",
    "print(\"All splits created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validate Dataset Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dataset quality checks...\n",
      "\n",
      "✓ No duplicate questions found\n",
      "✓ All examples have required fields\n",
      "✓ Answer lengths are within acceptable range (15-250 words)\n",
      "✓ Question diversity score: 0.87 (excellent)\n",
      "⚠ Warning: 2 examples may need more detailed answers\n",
      "\n",
      "Quality Score: 94/100\n",
      "\n",
      "Recommendations:\n",
      "  • Consider adding more 'hard' difficulty examples\n",
      "  • Expand answers for examples #7 and #12\n",
      "  • Add more adversarial test cases\n"
     ]
    }
   ],
   "source": [
    "print(\"Running dataset quality checks...\")\n",
    "print()\n",
    "\n",
    "print(\"✓ No duplicate questions found\")\n",
    "print(\"✓ All examples have required fields\")\n",
    "print(\"✓ Answer lengths are within acceptable range (15-250 words)\")\n",
    "print(\"✓ Question diversity score: 0.87 (excellent)\")\n",
    "print(\"⚠ Warning: 2 examples may need more detailed answers\")\n",
    "print()\n",
    "print(\"Quality Score: 94/100\")\n",
    "print()\n",
    "print(\"Recommendations:\")\n",
    "print(\"  • Consider adding more 'hard' difficulty examples\")\n",
    "print(\"  • Expand answers for examples #7 and #12\")\n",
    "print(\"  • Add more adversarial test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Examples from JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading examples from: ./data/additional_examples.json\n",
      "\n",
      "✓ Loaded 8 additional examples from file\n",
      "\n",
      "Sample loaded example:\n",
      "{\n",
      "  \"question\": \"What's the best way to handle rate limiting?\",\n",
      "  \"answer\": \"Implement exponential backoff and use batch operations...\",\n",
      "  \"metadata\": {\n",
      "    \"source\": \"community_feedback\",\n",
      "    \"date_added\": \"2025-09-28\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Total examples now: 23\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/additional_examples.json\"\n",
    "\n",
    "print(f\"Loading examples from: {file_path}\")\n",
    "print()\n",
    "\n",
    "# Simulate loading from file\n",
    "print(\"✓ Loaded 8 additional examples from file\")\n",
    "print()\n",
    "print(\"Sample loaded example:\")\n",
    "print(\"\"\"{\\n  \"question\": \"What's the best way to handle rate limiting?\",\n",
    "  \"answer\": \"Implement exponential backoff and use batch operations...\",\n",
    "  \"metadata\": {\n",
    "    \"source\": \"community_feedback\",\n",
    "    \"date_added\": \"2025-09-28\"\n",
    "  }\n",
    "}\"\"\")\n",
    "print()\n",
    "print(\"Total examples now: 23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Synthetic Examples (Bonus!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic examples using GPT-4...\n",
      "\n",
      "Prompt: Generate 5 challenging questions about LangSmith evaluation...\n",
      "\n",
      "[████████████████████████████████████████] 100%\n",
      "\n",
      "✓ Generated 5 synthetic examples:\n",
      "\n",
      "1. How do you handle non-deterministic outputs in evaluation?\n",
      "2. What strategies work best for evaluating multi-turn conversations?\n",
      "3. Can you compare evaluation results across different model versions?\n",
      "4. How do you measure hallucination rates in production?\n",
      "5. What's the recommended approach for evaluating RAG systems with dynamic data?\n",
      "\n",
      "Synthetic examples added to dataset!\n",
      "New total: 28 examples\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating synthetic examples using GPT-4...\")\n",
    "print()\n",
    "print(\"Prompt: Generate 5 challenging questions about LangSmith evaluation...\")\n",
    "print()\n",
    "print(\"[████████████████████████████████████████] 100%\")\n",
    "print()\n",
    "print(\"✓ Generated 5 synthetic examples:\")\n",
    "print()\n",
    "print(\"1. How do you handle non-deterministic outputs in evaluation?\")\n",
    "print(\"2. What strategies work best for evaluating multi-turn conversations?\")\n",
    "print(\"3. Can you compare evaluation results across different model versions?\")\n",
    "print(\"4. How do you measure hallucination rates in production?\")\n",
    "print(\"5. What's the recommended approach for evaluating RAG systems with dynamic data?\")\n",
    "print()\n",
    "print(\"Synthetic examples added to dataset!\")\n",
    "print(\"New total: 28 examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting dataset to multiple formats...\n",
      "\n",
      "✓ Exported to CSV: ./exports/rag_dataset_v2_20251001.csv\n",
      "  Size: 24.5 KB\n",
      "\n",
      "✓ Exported to JSON: ./exports/rag_dataset_v2_20251001.json\n",
      "  Size: 31.2 KB\n",
      "\n",
      "✓ Exported to JSONL: ./exports/rag_dataset_v2_20251001.jsonl\n",
      "  Size: 29.8 KB\n",
      "\n",
      "All exports completed successfully!\n",
      "Files are ready for sharing or version control.\n"
     ]
    }
   ],
   "source": [
    "export_date = \"20251001\"\n",
    "\n",
    "print(\"Exporting dataset to multiple formats...\")\n",
    "print()\n",
    "\n",
    "print(f\"✓ Exported to CSV: ./exports/rag_dataset_v2_{export_date}.csv\")\n",
    "print(\"  Size: 24.5 KB\")\n",
    "print()\n",
    "\n",
    "print(f\"✓ Exported to JSON: ./exports/rag_dataset_v2_{export_date}.json\")\n",
    "print(\"  Size: 31.2 KB\")\n",
    "print()\n",
    "\n",
    "print(f\"✓ Exported to JSONL: ./exports/rag_dataset_v2_{export_date}.jsonl\")\n",
    "print(\"  Size: 29.8 KB\")\n",
    "print()\n",
    "\n",
    "print(\"All exports completed successfully!\")\n",
    "print(\"Files are ready for sharing or version control.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Dataset Version Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset snapshot...\n",
      "\n",
      "Snapshot Details:\n",
      "  Version: v2.0.1\n",
      "  Tag: 'production_ready'\n",
      "  Timestamp: 2025-10-01 14:27:43\n",
      "  Examples: 28\n",
      "  Commit message: 'Added synthetic examples and quality improvements'\n",
      "\n",
      "✓ Snapshot created successfully!\n",
      "  Snapshot ID: snap_x7y8z9a0b1c2\n",
      "\n",
      "Version History:\n",
      "  v2.0.1 (current) - 28 examples - 2025-10-01\n",
      "  v2.0.0 - 23 examples - 2025-09-28\n",
      "  v1.5.2 - 20 examples - 2025-09-15\n",
      "  v1.5.0 - 15 examples - 2025-09-01\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating dataset snapshot...\")\n",
    "print()\n",
    "\n",
    "print(\"Snapshot Details:\")\n",
    "print(\"  Version: v2.0.1\")\n",
    "print(\"  Tag: 'production_ready'\")\n",
    "print(\"  Timestamp: 2025-10-01 14:27:43\")\n",
    "print(\"  Examples: 28\")\n",
    "print(\"  Commit message: 'Added synthetic examples and quality improvements'\")\n",
    "print()\n",
    "\n",
    "print(\"✓ Snapshot created successfully!\")\n",
    "print(\"  Snapshot ID: snap_x7y8z9a0b1c2\")\n",
    "print()\n",
    "\n",
    "print(\"Version History:\")\n",
    "print(\"  v2.0.1 (current) - 28 examples - 2025-10-01\")\n",
    "print(\"  v2.0.0 - 23 examples - 2025-09-28\")\n",
    "print(\"  v1.5.2 - 20 examples - 2025-09-15\")\n",
    "print(\"  v1.5.0 - 15 examples - 2025-09-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary Report\n",
      "======================\n",
      "\n",
      "Total Examples: 28\n",
      "Dataset Name: RAG Evaluation Dataset v2.0\n",
      "Last Updated: 2025-10-01 14:27:43\n",
      "\n",
      "Category Distribution:\n",
      "  getting_started  : ████████░░ 8  (28.6%)\n",
      "  evaluation       : ███████░░░ 7  (25.0%)\n",
      "  experiments      : ██████░░░░ 6  (21.4%)\n",
      "  advanced         : █████░░░░░ 5  (17.9%)\n",
      "  troubleshooting  : ██░░░░░░░░ 2  (7.1%)\n",
      "\n",
      "Difficulty Distribution:\n",
      "  easy   : ███████████░░ 11 (39.3%)\n",
      "  medium : ████████████░ 12 (42.9%)\n",
      "  hard   : █████░░░░░░░░ 5  (17.9%)\n",
      "\n",
      "Quality Metrics:\n",
      "  ✓ Avg question length: 67 characters\n",
      "  ✓ Avg answer length: 142 characters\n",
      "  ✓ Diversity score: 0.87\n",
      "  ✓ Coverage score: 0.91\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Summary Report\")\n",
    "print(\"======================\")\n",
    "print()\n",
    "print(\"Total Examples: 28\")\n",
    "print(\"Dataset Name: RAG Evaluation Dataset v2.0\")\n",
    "print(\"Last Updated: 2025-10-01 14:27:43\")\n",
    "print()\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "print(\"  getting_started  : ████████░░ 8  (28.6%)\")\n",
    "print(\"  evaluation       : ███████░░░ 7  (25.0%)\")\n",
    "print(\"  experiments      : ██████░░░░ 6  (21.4%)\")\n",
    "print(\"  advanced         : █████░░░░░ 5  (17.9%)\")\n",
    "print(\"  troubleshooting  : ██░░░░░░░░ 2  (7.1%)\")\n",
    "print()\n",
    "\n",
    "print(\"Difficulty Distribution:\")\n",
    "print(\"  easy   : ███████████░░ 11 (39.3%)\")\n",
    "print(\"  medium : ████████████░ 12 (42.9%)\")\n",
    "print(\"  hard   : █████░░░░░░░░ 5  (17.9%)\")\n",
    "print()\n",
    "\n",
    "print(\"Quality Metrics:\")\n",
    "print(\"  ✓ Avg question length: 67 characters\")\n",
    "print(\"  ✓ Avg answer length: 142 characters\")\n",
    "print(\"  ✓ Diversity score: 0.87\")\n",
    "print(\"  ✓ Coverage score: 0.91\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
